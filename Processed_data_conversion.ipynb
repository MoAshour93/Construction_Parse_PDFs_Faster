{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "* This file is about a github repository that allows extracting text from pdfs. The link to the github repository is https://github.com/VikParuchuri/marker?tab=readme-ov-file.\n",
    "* The main package is called marker-pdf. You can install it using pip install marker-pdf.\n",
    "* Be careful that it installs by default torch with cpu support only. Make sure to open the terminal in the folder where you are going to work and save your pdfs and the parsed markdown.\n",
    "* Create a virtual environment first \" *python -m venv parse_pdfs* \" then type \" *parse_pdfs\\Scripts\\activate* \" to activate the virtual environment. After that you can install easily the marker-pdf package.\n",
    "* If you have an Nvidia GPU, do not forget to go to *www.pytorch.org* and install the latest pytorch with cuda support using the following command in your virtual environment \" *pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121* \"\n",
    "\n",
    "### Convert a single file\n",
    "* You can convert a single file by following this command : \" *marker_single /path/to/file.pdf /path/to/output/folder --batch_multiplier 2 --max_pages 10 --langs English* \"\n",
    "* --batch_multiplier is how much to multiply default batch sizes by if you have extra VRAM. Higher numbers will take more VRAM, but process faster. Set to 2 by default. The default batch sizes will take ~3GB of VRAM.\n",
    "* --max_pages is the maximum number of pages to process. Omit this to convert the entire document.\n",
    "* --langs is a comma separated list of the languages in the document, for OCR\n",
    "* In my experience, I followed the following command and it was enough for my use case : \" *marker_single /path/to/file.pdf /path/to/output/folder* \"\n",
    "\n",
    "### Convert multiple files\n",
    "* You can convert multiple files by following this command: \" *marker /path/to/input/folder /path/to/output/folder --workers 10 --max 10 --metadata_file /path/to/metadata.json --min_length 10000* \"\n",
    "* --workers is the number of pdfs to convert at once. This is set to 1 by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Parallelism will not increase beyond INFERENCE_RAM / VRAM_PER_TASK if you're using GPU.\n",
    "* --max is the maximum number of pdfs to convert. Omit this to convert all pdfs in the folder.\n",
    "* --min_length is the minimum number of characters that need to be extracted from a pdf before it will be considered for processing. If you're processing a lot of pdfs, I recommend setting this to avoid OCRing pdfs that are mostly images. (slows everything down)\n",
    "* --metadata_file is an optional path to a json file with metadata about the pdfs. If you provide it, it will be used to set the language for each pdf. If not, DEFAULT_LANG will be used. \n",
    "* In my experience, I followed the following command and it was enough for my use case: \" *marker /path/to/input/folder /path/to/output/folder* \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved as: D:/Documents/Dossiers du travail/Technical Excellence/Chatbots/Created chatbots/Parse PDFs faster/converted_pdfs/Raouf_SoE_RICS_APC/Raouf_SoE_RICS_APC_processed_v1.jsonl\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Function to choose multiple input files\n",
    "def choose_files():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select Markdown files\",\n",
    "        filetypes=((\"Markdown files\", \"*.md\"), (\"All files\", \"*.*\"))\n",
    "    )\n",
    "    return file_paths\n",
    "\n",
    "# Function to choose the location to save the output file\n",
    "def save_file_dialog():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    output_file_path = filedialog.asksaveasfilename(\n",
    "        title=\"Save Processed File\",\n",
    "        defaultextension=\".jsonl\",\n",
    "        filetypes=((\"JSON Lines files\", \"*.jsonl\"), (\"All files\", \"*.*\"))\n",
    "    )\n",
    "    return output_file_path\n",
    "\n",
    "# Function to process the content by removing unwanted text and adjusting spacing\n",
    "def process_content(content):\n",
    "    content = re.sub(r'!\\[\\d+_image_\\d+\\.png\\]\\(\\d+_image_\\d+\\.png\\)', '', content)  # Remove image references\n",
    "    content = content.replace(\"assessment Not\", \"\")  # Remove specific text\n",
    "    content = content.replace(\"Not\", \"\")  # Remove specific text\n",
    "    content = content.replace(\"Not assessment\", \"\")  # Remove specific text\n",
    "    content = content.replace(\"for assessment\", \"\")  # Remove specific text\n",
    "    content = re.sub(r'\\n\\s*\\n', '\\n\\n', content)  # Remove extra blank lines\n",
    "    return content\n",
    "\n",
    "# Function to extract labeled data from the content\n",
    "def extract_labeled_data(content):\n",
    "    data = []\n",
    "    # Split content into sections based on the competency type\n",
    "    sections = re.split(r'##\\s+(Mandatory|Core|Optional) Competencies', content)\n",
    "    \n",
    "    #Creating an array containing the title of the competency group in an entry and all the competencies within each group in another entry\n",
    "    # e.g. 3 entries for competencies groups and 3 entries for all the competencies within every group\n",
    "    for i in range(1, len(sections), 2):\n",
    "        section = sections[i]\n",
    "        section_content = sections[i+1]\n",
    "        current_section = {\"level\": 1, \"text\": section}\n",
    "        data.append(current_section)\n",
    "        \n",
    "        # Split section content into 3 levels, level 1 - competency group , level 2 - name of the competency , level 3 - each level in the competency\n",
    "        competencies = re.split(r'Competency:', section_content)\n",
    "        \n",
    "        for competency in competencies[1:]:\n",
    "            competency_lines = competency.strip().split('\\n')\n",
    "            competency_name = competency_lines[0].strip()\n",
    "            current_competency = {\"level\": 2, \"text\": \"Competency: \" + competency_name}\n",
    "            data.append(current_competency)\n",
    "            \n",
    "            current_level = None\n",
    "            for line in competency_lines[1:]:\n",
    "                if line.startswith(\"Level:\"):\n",
    "                    if current_level:\n",
    "                        data.append(current_level)\n",
    "                    current_level = {\"level\": 3, \"text\": line.strip()}\n",
    "                else:\n",
    "                    if current_level:\n",
    "                        current_level[\"text\"] += \" \" + line.strip()\n",
    "            if current_level:\n",
    "                data.append(current_level)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to create a conversational structure from the labeled data\n",
    "def create_conversational_structure(data):\n",
    "    conversations = []\n",
    "    human_prompt = \"<human>\"\n",
    "    bot_response = \"<bot>\"\n",
    "\n",
    "    competencies = {}\n",
    "    current_competency = None\n",
    "\n",
    "    # Organize the data into competencies and their respective levels\n",
    "    for item in data:\n",
    "        if item['level'] == 1:\n",
    "            current_section = item['text']\n",
    "        elif item['level'] == 2:\n",
    "            current_competency = item['text']\n",
    "            if current_competency not in competencies:\n",
    "                competencies[current_competency] = {\"section\": current_section, \"levels\": []}\n",
    "        elif item['level'] == 3:\n",
    "            if current_competency:\n",
    "                competencies[current_competency]['levels'].append(item['text']) \n",
    "\n",
    "    # Find the latest level for each competency\n",
    "    latest_levels = {}\n",
    "    for competency, details in competencies.items():\n",
    "        latest_level = 0\n",
    "        for level_text in details['levels']:\n",
    "            level_match = re.search(r'Level:\\s*(\\d+)', level_text)\n",
    "            if level_match:\n",
    "                level = int(level_match.group(1))\n",
    "                if level > latest_level:\n",
    "                    latest_level = level\n",
    "        latest_levels[competency] = latest_level\n",
    "\n",
    "    # Create conversation structure for each level of each competency\n",
    "    for competency, details in competencies.items():\n",
    "        for level_text in details['levels']:\n",
    "            level_match = re.search(r'Level:\\s*(\\d+)', level_text)\n",
    "            level = level_match.group(1) if level_match else \"unknown\"\n",
    "            latest_level = latest_levels[competency]\n",
    "            conversation = {\n",
    "                \"text\": (\n",
    "                    f\"{human_prompt} I want to write about {competency}\\n\"\n",
    "                    f\"{bot_response} Sure I can help with that. What level are you seeking?\\n\"\n",
    "                    f\"{human_prompt} How many levels are there in this competency?\\n\"\n",
    "                    f\"{bot_response} Based on the information that I have, the latest level for this {competency} competency is {latest_level}.\\n\"\n",
    "                    f\"{human_prompt} Okay, you could help me with level {level}.\\n\"\n",
    "                    f\"{bot_response} Sure, here is a proposed response: {level_text}\\n\"\n",
    "                )\n",
    "            }\n",
    "            conversations.append(conversation)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Function to save the conversations as a JSON Lines file\n",
    "def save_conversations_as_jsonl(conversations, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for conversation in conversations:\n",
    "            file.write(json.dumps(conversation) + '\\n')\n",
    "\n",
    "# Main function to execute the overall process\n",
    "def main():\n",
    "    file_paths = choose_files()\n",
    "    if file_paths:\n",
    "        all_data = []\n",
    "        for file_path in file_paths:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                processed_content = process_content(content)\n",
    "                labeled_data = extract_labeled_data(processed_content)\n",
    "                all_data.extend(labeled_data)\n",
    "\n",
    "        conversations = create_conversational_structure(all_data)\n",
    "\n",
    "        output_file_path = save_file_dialog()\n",
    "        if output_file_path:\n",
    "            save_conversations_as_jsonl(conversations, output_file_path)\n",
    "            print(f\"Processed file saved as: {output_file_path}\")\n",
    "        else:\n",
    "            print(\"Save operation cancelled.\")\n",
    "    else:\n",
    "        print(\"No files selected.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
